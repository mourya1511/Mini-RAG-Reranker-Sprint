Overview

This project deploys a small Retrieval-Augmented Generation (RAG) pipeline across 20 public PDFs of industrial and machine safety. It offers:

Document ingestion and chunking (paragraph-level)

Computation of embeddings with all-MiniLM-L6-v2 and FAISS indexing

Baseline similarity search via cosine similarity

Hybrid reranker blending vector similarity and keyword overlap

Simple API (/ask) for extractive, cited answers

Testing via 8 example questions

The aim is a reproducible, truthful Q&A service where unsupported queries lead to abstention.

Project Structure
mini-rag/
├─ pdfs/                   # 20 PDFs
├─ data/
│  ├─ chunks.db            # generated by ingest.py
│  └─ embeddings/          # FAISS index + embeddings
├─ ingest.py
├─ embed_index.py
├─ baseline_search.py
├─ reranker.py
├─ api.py
├─ eval.py
├─ 8_questions.json
├─ requirements.txt
└─ README.md

Setup Instructions

Clone repo and unzip PDFs

mkdir mini-rag && cd mini-rag
unzip ./industrial-safety-pdfs.zip -d pdfs

Create virtual environment and install dependencies

python3 -m venv .venv
source .venv/bin/activate   # Windows: .\\.venv\\Scripts\\Activate.ps1
pip install -r requirements.txt

Run ingestion

python ingest.py

Compute embeddings and build FAISS index

python embed_index.py

Start API server

uvicorn api:app --reload

API Usage

Easy query
curl -X POST "http://localhost:8000/ask" \
-H "Content-Type: application/json" \
-d '{"q":"What are lockout/tagout steps?","k":5,"mode":"hybrid"}'

Tricky query (might abstain)
curl -X POST "http://localhost:8000/ask" \
-H "Content-Type: application/json" \
-d '{"q": "Does OSHA require purple helmets for machine shops?", "k":5,"mode":"hybrid"}'

Response includes:
answer: extractive answer or null if abstained
contexts: retrieved chunks with scores and metadata
reranker_used: baseline or hybrid

Evaluation
Run the 8 example questions:
python eval.py

Sample results:
Question	Baseline Answer	Hybrid Answer	Top Combined Score
PPE near conveyors?	Gloves, helmets.	Gloves, helmets.	0.87
Lockout/tagout steps?	Step 1.	Step 1.	0.92
Purple helmets required?	N/A	N/A	0.12

The hybrid reranker enhanced precision in most queries; abstention prevented unsupported answers.

How it Works
Ingest: PDFs → text → paragraph chunks → SQLite

Embedding: Chunks → embeddings → FAISS index

Baseline Search: Cosine similarity → top-k chunks

Hybrid Reranker: Weighted combination of normalized vector + keyword scores → top evidence rises

Answer Selection: Extractive sentence with highest overlap with query; abstain if score < threshold

Lessons Learned

Blending semantic embeddings with keyword overlap enhances relevance

A threshold prevents abstention on unsupported queries

Local models such as all-MiniLM-L6-v2 suffice for small-scale RAG

Chunking and normalization appropriately are important for retrieval quality

Tuning ALPHA (reranker weight) and THRESHOLD (abstain control) maximizes accuracy

Notes / Tips

Tune hybrid scoring by adjusting ALPHA in reranker.py

Control abstention by adjusting THRESHOLD in api.py

Save data/embeddings and data/chunks.db for reproducibility
